{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 7618922,
          "sourceType": "datasetVersion",
          "datasetId": 4437625
        },
        {
          "sourceId": 164521106,
          "sourceType": "kernelVersion"
        }
      ],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "LLM_dL_notebook",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'llm-datasets:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4437625%2F7618922%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240227%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240227T114347Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D13b09b03bf50ad8d74e05c6ac6da319be3ad0340e28c0a75996d522a0c06a796a172b1092c94d1539ff8f88dc1f70738064ace892cd37f01081066d513332661ae31cc850e65cd0a857b6d74e198ac9211f30cba4f00f1c2a6b6080bc4cea888c11abd3154bd5d1bca0fdfd200450bb15d50a2a43dcccb186c9f8c94154a48a4e1a11775d337b8450b919a22acaaa7d857dada3ae0537557f82df934c407d2670c1799181d2b81dea82066e9e41a29575ea7f65ad2a8de51876e9d41da396d33e7bdecfbbf95a8003ed92626a362e1d851525cb521c9e51d9fb6f4d6bb4f93bab7a52e69c683e19cdf4a5c65f9a833f83a68d358dd6184e7815d1f1bb183a6fa,llm-model:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4503294%2F7711889%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240227%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240227T114347Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3e80627b72ba2271578dbbf149d5182fc1cbb71a26145491de6c12833121d5b069d526decb9db08faef84b19c4e31770c73455a5e6c1dc5cf42af01197d57839a14e7960f4e34a4568cfb2e58d0805b9fb79e6831d0d152c815c9cf684b1126a8ad9fae4bb11dceda1529fa1fea6431e9b7651de99b1ff3a2a3c278f0a37e813104429852e27432bd9d5a7cbabccd81ee66abe222ea50f6faf8a0a4e2ebf956b4e0f2bd1a9f8a6fe7eff700541739731a8a95cad1c752f502b46744d1e36398a2ca2b223b81f6afdbaad5b8d2b6024949774cc8e8275b9d8418da176904ea45d875ff2d7f8d50597f846d925955c5c5ef1af68d371b2f481dd8e398e146006df'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "DoV0GOsjYMK9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-02-27T08:51:52.140625Z",
          "iopub.execute_input": "2024-02-27T08:51:52.140977Z",
          "iopub.status.idle": "2024-02-27T08:51:53.0401Z",
          "shell.execute_reply.started": "2024-02-27T08:51:52.140949Z",
          "shell.execute_reply": "2024-02-27T08:51:53.039044Z"
        },
        "trusted": true,
        "id": "toss9l4VYMK-",
        "outputId": "4ec00e42-9f5c-4cd2-c497-4816f3ea661d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/llm-datasets/sample_submission.csv\n/kaggle/input/llm-datasets/train.csv\n/kaggle/input/llm-datasets/test.csv\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!pip install kaggle --upgrade\n",
        "!pip install wandb\n",
        "!pip install peft\n",
        "!pip install bitsandbytes\n",
        "# !pip install transformers accelerate bitsandbytes\n",
        "#!pip install git+https://github.com/zhanghang1989/PyTorch-Encoding/"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:51:53.042178Z",
          "iopub.execute_input": "2024-02-27T08:51:53.043019Z",
          "iopub.status.idle": "2024-02-27T08:52:33.293633Z",
          "shell.execute_reply.started": "2024-02-27T08:51:53.042983Z",
          "shell.execute_reply": "2024-02-27T08:52:33.292699Z"
        },
        "trusted": true,
        "id": "2VM65hr2YMK_",
        "outputId": "4ef6f559-e3da-4d69-db3c-0485a263d5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting gdown\n  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.11.17)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.1.0-py3-none-any.whl (17 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.1.0\nRequirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (1.6.3)\nCollecting kaggle\n  Downloading kaggle-1.6.6.tar.gz (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kaggle) (2023.11.17)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.66.1)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle) (8.0.1)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.18)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle) (6.1.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.6)\nBuilding wheels for collected packages: kaggle\n  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.6-py3-none-any.whl size=111943 sha256=87cb51c1f06b8add5ce3955093a82c7b4948c4b80416e4cc87956e61d408e8bd\n  Stored in directory: /root/.cache/pip/wheels/53/34/8c/8ca3450d17206d9e37e1ee3aeb47cbb2873d22a9e0c60eb137\nSuccessfully built kaggle\nInstalling collected packages: kaggle\n  Attempting uninstall: kaggle\n    Found existing installation: kaggle 1.6.3\n    Uninstalling kaggle-1.6.3:\n      Successfully uninstalled kaggle-1.6.3\nSuccessfully installed kaggle-1.6.6\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.39.2)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import bisect\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import transformers\n",
        "import bitsandbytes as bnb\n",
        "from transformers import PreTrainedTokenizerFast, AdamW, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizerFast, GPT2LMHeadModel\n",
        "from transformers import ElectraModel, ElectraTokenizer\n",
        "#from parallel import DataParallelModel, DataParallelCriterion\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from nltk.corpus import stopwords\n",
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AdamW\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "!git clone https://github.com/ttony0321/stopwords.txt.git\n",
        "#os.environ['WANDB_DISABLED'] = \"false\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:33.294969Z",
          "iopub.execute_input": "2024-02-27T08:52:33.295238Z",
          "iopub.status.idle": "2024-02-27T08:52:45.987506Z",
          "shell.execute_reply.started": "2024-02-27T08:52:33.295213Z",
          "shell.execute_reply": "2024-02-27T08:52:45.986296Z"
        },
        "trusted": true,
        "id": "Fvx1cCoGYMK_",
        "outputId": "b5cf5fed-b8b0-4c10-cbf9-97bfc7fdbcfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Cloning into 'stopwords.txt'...\nremote: Enumerating objects: 6, done.\u001b[K\nremote: Counting objects: 100% (6/6), done.\u001b[K\nremote: Compressing objects: 100% (4/4), done.\u001b[K\nremote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (6/6), 4.29 KiB | 4.29 MiB/s, done.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/file/d/1ySDZrZlDfkxdGOt_h4zofIHJNhkUq5oB/view?usp=sharing\n",
        "#!gdown 1ySDZrZlDfkxdGOt_h4zofIHJNhkUq5oB\n",
        "#if gdown doesn't work\n",
        "#!unzip /kaggle/working/open.zip"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:45.99044Z",
          "iopub.execute_input": "2024-02-27T08:52:45.991569Z",
          "iopub.status.idle": "2024-02-27T08:52:45.996935Z",
          "shell.execute_reply.started": "2024-02-27T08:52:45.99154Z",
          "shell.execute_reply": "2024-02-27T08:52:45.995931Z"
        },
        "trusted": true,
        "id": "GRdn84k1YMK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:45.998198Z",
          "iopub.execute_input": "2024-02-27T08:52:45.998504Z",
          "iopub.status.idle": "2024-02-27T08:52:47.011776Z",
          "shell.execute_reply.started": "2024-02-27T08:52:45.998479Z",
          "shell.execute_reply": "2024-02-27T08:52:47.010772Z"
        },
        "trusted": true,
        "id": "XY95mIzpYMK_",
        "outputId": "f6dc41ab-03be-4f72-99d2-3d547945f70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Tue Feb 27 08:52:46 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0              25W / 250W |      2MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0,1\"  # Set the GPU 2 to use"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.013205Z",
          "iopub.execute_input": "2024-02-27T08:52:47.0135Z",
          "iopub.status.idle": "2024-02-27T08:52:47.018833Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.013473Z",
          "shell.execute_reply": "2024-02-27T08:52:47.017828Z"
        },
        "trusted": true,
        "id": "waAxW3SGYMK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('Device:', device)\n",
        "print('Current cuda device:', torch.cuda.current_device())\n",
        "print('Count of using GPUs:', torch.cuda.device_count())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.020048Z",
          "iopub.execute_input": "2024-02-27T08:52:47.020335Z",
          "iopub.status.idle": "2024-02-27T08:52:47.033268Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.020311Z",
          "shell.execute_reply": "2024-02-27T08:52:47.032289Z"
        },
        "trusted": true,
        "id": "gmgU-cRXYMLA",
        "outputId": "5171a55a-46e8-4966-a572-466b70b6d8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Device: cuda\nCurrent cuda device: 0\nCount of using GPUs: 1\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train = pd.read_csv('/kaggle/working/train.csv')\n",
        "# test = pd.read_csv('/kaggle/working/train.csv')\n",
        "#if not gdown\n",
        "train = pd.read_csv('/kaggle/input/llm-datasets/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/llm-datasets/test.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.034524Z",
          "iopub.execute_input": "2024-02-27T08:52:47.034811Z",
          "iopub.status.idle": "2024-02-27T08:52:47.113807Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.03478Z",
          "shell.execute_reply": "2024-02-27T08:52:47.113091Z"
        },
        "trusted": true,
        "id": "GtLQhf8NYMLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.115053Z",
          "iopub.execute_input": "2024-02-27T08:52:47.115491Z",
          "iopub.status.idle": "2024-02-27T08:52:47.137431Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.115454Z",
          "shell.execute_reply": "2024-02-27T08:52:47.136566Z"
        },
        "trusted": true,
        "id": "yNTK2iPCYMLA",
        "outputId": "967369b4-76a4-458a-b6c4-49a0a24fbaf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "          id               질문_1                               질문_2 category  \\\n0  TRAIN_000          면진장치가 뭐야?           면진장치에 사용되는 주요 기술은 무엇인가요?     건축구조   \n1  TRAIN_001     내진설계의 종류 좀 알려줘  내진설계에는 어떤 종류가 있는지 자세히 알려주실 수 있나요?     건축구조   \n2  TRAIN_002      철골구조의 장점이 뭐야?                     철골구조의 장점을 알려줘?     건축구조   \n3  TRAIN_003  철골철근 콘크리트 구조가 뭐야?   철골철근 콘크리트 구조의 장점과 단점에는 무엇이 있을까요?     건축구조   \n4  TRAIN_004   철골구조는 어떤 방식이 있어?               철골구조의 다양한 방식이 무엇인가요?     건축구조   \n\n                                                답변_1  \\\n0  면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 ...   \n1                 내진 설계의 종류로 내진구조, 제진구조, 면진구조가 있습니다.   \n2  철골구조는 건물의 외벽에는 그다지 하중이 걸리지 않기 때문에 고층 건물의 건축이 가...   \n3  철근철골콘크리트는 철골과 철근, 그리고 콘크리트를 함께 사용하는 건축 구조입니다. ...   \n4                        철골구조는 일반철골구조와 경량철골구조가 있습니다.   \n\n                                                답변_2  \\\n0  면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고, 진동을...   \n1  내진설계에는 내진구조, 제진구조, 면진구조가 있습니다. 내진구조는 건물 구조물이 지...   \n2  철골구조의 장점은 건물의 외벽에는 그다지 하중이 걸리지 않기 때문에 고층 건물의 건...   \n3  철골철근콘크리트 구조는 건축물을 지탱하는 주요 구조물인 철골과 철근, 그리고 콘크리...   \n4  철골구조는 일반철골구조와 경량철골구조가 있습니다. 일반철골구조는 주로 대형 건물이나...   \n\n                                                답변_3  \\\n0  면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여...   \n1  내진설계에는 주로 내진구조, 제진구조, 면진구조의 세 가지 종류가 있습니다. 이들은...   \n2  철골구조의 장점은 건물의 외벽에 하중이 적게 걸리기 때문에 고층 건물의 건축이 용이...   \n3  철골철근 콘크리트 구조는 건축물을 지탱하기 위한 구조물에서 일반적으로 사용되는 방식...   \n4  철골구조는 주로 일반철골구조와 경량철골구조로 나뉘어집니다. 이들은 건축 시스템에 따...   \n\n                                                답변_4  \\\n0  면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 ...   \n1  내진설계에는 주로 내진구조, 제진구조, 면진구조가 사용됩니다. 내진구조는 건물 구조...   \n2  철골구조의 장점은 건물의 외벽이 하중이 걸리지 않아 공간 활용이 용이하고, 고층 건...   \n3  철골철근콘크리트 구조는 철골과 철근, 그리고 콘크리트를 함께 사용하여 만들어지는 건...   \n4  철골구조는 주로 일반철골구조와 경량철골구조로 구분됩니다. 이외에도 최근에는 고층 건...   \n\n                                                답변_5  \n0  면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동...  \n1  내진 설계에는 다양한 종류가 있지만, 대표적으로 내진구조, 제진구조, 면진구조가 있...  \n2  철골구조의 장점은 건물의 외벽에 하중이 크게 걸리지 않아 고층 건물을 건축할 수 있...  \n3  철골철근 콘크리트 구조는 강철 골조와 강철 철근, 그리고 콘크리트를 함께 사용하여 ...  \n4  철골구조는 일반철골구조와 경량철골구조 두 가지 방식이 주로 사용됩니다. 일반철골구조...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>질문_1</th>\n      <th>질문_2</th>\n      <th>category</th>\n      <th>답변_1</th>\n      <th>답변_2</th>\n      <th>답변_3</th>\n      <th>답변_4</th>\n      <th>답변_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAIN_000</td>\n      <td>면진장치가 뭐야?</td>\n      <td>면진장치에 사용되는 주요 기술은 무엇인가요?</td>\n      <td>건축구조</td>\n      <td>면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 ...</td>\n      <td>면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고, 진동을...</td>\n      <td>면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여...</td>\n      <td>면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 ...</td>\n      <td>면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRAIN_001</td>\n      <td>내진설계의 종류 좀 알려줘</td>\n      <td>내진설계에는 어떤 종류가 있는지 자세히 알려주실 수 있나요?</td>\n      <td>건축구조</td>\n      <td>내진 설계의 종류로 내진구조, 제진구조, 면진구조가 있습니다.</td>\n      <td>내진설계에는 내진구조, 제진구조, 면진구조가 있습니다. 내진구조는 건물 구조물이 지...</td>\n      <td>내진설계에는 주로 내진구조, 제진구조, 면진구조의 세 가지 종류가 있습니다. 이들은...</td>\n      <td>내진설계에는 주로 내진구조, 제진구조, 면진구조가 사용됩니다. 내진구조는 건물 구조...</td>\n      <td>내진 설계에는 다양한 종류가 있지만, 대표적으로 내진구조, 제진구조, 면진구조가 있...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAIN_002</td>\n      <td>철골구조의 장점이 뭐야?</td>\n      <td>철골구조의 장점을 알려줘?</td>\n      <td>건축구조</td>\n      <td>철골구조는 건물의 외벽에는 그다지 하중이 걸리지 않기 때문에 고층 건물의 건축이 가...</td>\n      <td>철골구조의 장점은 건물의 외벽에는 그다지 하중이 걸리지 않기 때문에 고층 건물의 건...</td>\n      <td>철골구조의 장점은 건물의 외벽에 하중이 적게 걸리기 때문에 고층 건물의 건축이 용이...</td>\n      <td>철골구조의 장점은 건물의 외벽이 하중이 걸리지 않아 공간 활용이 용이하고, 고층 건...</td>\n      <td>철골구조의 장점은 건물의 외벽에 하중이 크게 걸리지 않아 고층 건물을 건축할 수 있...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRAIN_003</td>\n      <td>철골철근 콘크리트 구조가 뭐야?</td>\n      <td>철골철근 콘크리트 구조의 장점과 단점에는 무엇이 있을까요?</td>\n      <td>건축구조</td>\n      <td>철근철골콘크리트는 철골과 철근, 그리고 콘크리트를 함께 사용하는 건축 구조입니다. ...</td>\n      <td>철골철근콘크리트 구조는 건축물을 지탱하는 주요 구조물인 철골과 철근, 그리고 콘크리...</td>\n      <td>철골철근 콘크리트 구조는 건축물을 지탱하기 위한 구조물에서 일반적으로 사용되는 방식...</td>\n      <td>철골철근콘크리트 구조는 철골과 철근, 그리고 콘크리트를 함께 사용하여 만들어지는 건...</td>\n      <td>철골철근 콘크리트 구조는 강철 골조와 강철 철근, 그리고 콘크리트를 함께 사용하여 ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRAIN_004</td>\n      <td>철골구조는 어떤 방식이 있어?</td>\n      <td>철골구조의 다양한 방식이 무엇인가요?</td>\n      <td>건축구조</td>\n      <td>철골구조는 일반철골구조와 경량철골구조가 있습니다.</td>\n      <td>철골구조는 일반철골구조와 경량철골구조가 있습니다. 일반철골구조는 주로 대형 건물이나...</td>\n      <td>철골구조는 주로 일반철골구조와 경량철골구조로 나뉘어집니다. 이들은 건축 시스템에 따...</td>\n      <td>철골구조는 주로 일반철골구조와 경량철골구조로 구분됩니다. 이외에도 최근에는 고층 건...</td>\n      <td>철골구조는 일반철골구조와 경량철골구조 두 가지 방식이 주로 사용됩니다. 일반철골구조...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['category'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.140963Z",
          "iopub.execute_input": "2024-02-27T08:52:47.141227Z",
          "iopub.status.idle": "2024-02-27T08:52:47.155999Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.141205Z",
          "shell.execute_reply": "2024-02-27T08:52:47.155058Z"
        },
        "trusted": true,
        "id": "77yXduihYMLA",
        "outputId": "31d7a3a6-8c00-4f6c-cdb8-8c102cd10bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "category\n마감재       272\n인테리어      123\n시공        111\n마감하자       60\n건축구조       31\n기타         27\n타 마감하자     20\nName: count, dtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_kr = '/kaggle/working/stopwords.txt/stopwords.txt'\n",
        "with open(stopwords_kr, 'r') as file:\n",
        "    read_line = file.read()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.157091Z",
          "iopub.execute_input": "2024-02-27T08:52:47.157355Z",
          "iopub.status.idle": "2024-02-27T08:52:47.161837Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.157333Z",
          "shell.execute_reply": "2024-02-27T08:52:47.161041Z"
        },
        "trusted": true,
        "id": "k5Wkx5wYYMLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = read_line.replace('\\n', ' ')\n",
        "stop_words = set(stop_words.split(' '))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.163065Z",
          "iopub.execute_input": "2024-02-27T08:52:47.163372Z",
          "iopub.status.idle": "2024-02-27T08:52:47.170343Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.163347Z",
          "shell.execute_reply": "2024-02-27T08:52:47.169581Z"
        },
        "trusted": true,
        "id": "KW4PrjM_YMLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess\n",
        "def remove_spec(text):\n",
        "    #text = re.sub('[^가-힣a-z]', ' ', text)\n",
        "    text = re.sub(r'[^가-힣A-Za-z0-9 ]', '', text)#특수문자 제거\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()#공백 하나\n",
        "    return text\n",
        "#stop words\n",
        "def stopw(text):\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if not word in stop_words]\n",
        "    cleaned_text = \" \".join(tokens)\n",
        "    return cleaned_text\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.17153Z",
          "iopub.execute_input": "2024-02-27T08:52:47.172073Z",
          "iopub.status.idle": "2024-02-27T08:52:47.183228Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.172042Z",
          "shell.execute_reply": "2024-02-27T08:52:47.182336Z"
        },
        "trusted": true,
        "id": "N9NSEAK0YMLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['질문_1'] = train['질문_1'].apply(remove_spec)\n",
        "train['질문_2'] = train['질문_2'].apply(remove_spec)\n",
        "\n",
        "train['답변_1'] = train['답변_1'].apply(remove_spec)\n",
        "train['답변_2'] = train['답변_2'].apply(remove_spec)\n",
        "train['답변_3'] = train['답변_3'].apply(remove_spec)\n",
        "train['답변_4'] = train['답변_4'].apply(remove_spec)\n",
        "train['답변_5'] = train['답변_5'].apply(remove_spec)\n",
        "\n",
        "train['질문_1'] = train['질문_1'].apply(stopw)\n",
        "train['질문_2'] = train['질문_2'].apply(stopw)\n",
        "\n",
        "train['답변_1'] = train['답변_1'].apply(stopw)\n",
        "train['답변_2'] = train['답변_2'].apply(stopw)\n",
        "train['답변_3'] = train['답변_3'].apply(stopw)\n",
        "train['답변_4'] = train['답변_4'].apply(stopw)\n",
        "train['답변_5'] = train['답변_5'].apply(stopw)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.184336Z",
          "iopub.execute_input": "2024-02-27T08:52:47.18461Z",
          "iopub.status.idle": "2024-02-27T08:52:47.3416Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.184588Z",
          "shell.execute_reply": "2024-02-27T08:52:47.340815Z"
        },
        "trusted": true,
        "id": "9trCjW2kYMLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.342604Z",
          "iopub.execute_input": "2024-02-27T08:52:47.342904Z",
          "iopub.status.idle": "2024-02-27T08:52:47.356155Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.34288Z",
          "shell.execute_reply": "2024-02-27T08:52:47.355183Z"
        },
        "trusted": true,
        "id": "zLr_ls5UYMLB",
        "outputId": "5b6a1343-9158-46fe-b2e3-3e4477d77f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": "          id              질문_1                             질문_2 category  \\\n0  TRAIN_000          면진장치가 뭐야          면진장치에 사용되는 주요 기술은 무엇인가요     건축구조   \n1  TRAIN_001      내진설계의 종류 알려줘      내진설계에는 종류가 있는지 자세히 알려주실 있나요     건축구조   \n2  TRAIN_002      철골구조의 장점이 뭐야                    철골구조의 장점을 알려줘     건축구조   \n3  TRAIN_003  철골철근 콘크리트 구조가 뭐야  철골철근 콘크리트 구조의 장점과 단점에는 무엇이 있을까요     건축구조   \n4  TRAIN_004      철골구조는 방식이 있어              철골구조의 다양한 방식이 무엇인가요     건축구조   \n\n                                                답변_1  \\\n0  면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 ...   \n1                    내진 설계의 종류로 내진구조 제진구조 면진구조가 있습니다   \n2  철골구조는 건물의 외벽에는 그다지 하중이 걸리지 고층 건물의 건축이 가능한 것이 장...   \n3  철근철골콘크리트는 철골과 철근 콘크리트를 사용하는 건축 구조입니다 철골은 강하고 가...   \n4                         철골구조는 일반철골구조와 경량철골구조가 있습니다   \n\n                                                답변_2  \\\n0  면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고 진동을 ...   \n1  내진설계에는 내진구조 제진구조 면진구조가 있습니다 내진구조는 건물 구조물이 지진의 ...   \n2  철골구조의 장점은 건물의 외벽에는 그다지 하중이 걸리지 고층 건물의 건축이 가능하다...   \n3  철골철근콘크리트 구조는 건축물을 지탱하는 주요 구조물인 철골과 철근 콘크리트를 사용...   \n4  철골구조는 일반철골구조와 경량철골구조가 있습니다 일반철골구조는 주로 대형 건물이나 ...   \n\n                                                답변_3  \\\n0  면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여...   \n1  내진설계에는 주로 내진구조 제진구조 면진구조의 세 가지 종류가 있습니다 이들은 지진...   \n2  철골구조의 장점은 건물의 외벽에 하중이 적게 걸리기 고층 건물의 건축이 용이하다는 ...   \n3  철골철근 콘크리트 구조는 건축물을 지탱하기 위한 구조물에서 사용되는 방식으로 철골을...   \n4  철골구조는 주로 일반철골구조와 경량철골구조로 나뉘어집니다 이들은 건축 시스템에 선적...   \n\n                                                답변_4  \\\n0  면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 ...   \n1  내진설계에는 주로 내진구조 제진구조 면진구조가 사용됩니다 내진구조는 건물 구조물 전...   \n2  철골구조의 장점은 건물의 외벽이 하중이 걸리지 않아 공간 활용이 용이하고 고층 건물...   \n3  철골철근콘크리트 구조는 철골과 철근 콘크리트를 사용하여 만들어지는 건축 구조입니다 ...   \n4  철골구조는 주로 일반철골구조와 경량철골구조로 구분됩니다 최근에는 고층 건물에서 사용...   \n\n                                                답변_5  \n0  면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동...  \n1  내진 설계에는 다양한 종류가 있지만 대표적으로 내진구조 제진구조 면진구조가 있습니다...  \n2  철골구조의 장점은 건물의 외벽에 하중이 크게 걸리지 않아 고층 건물을 건축할 있는 ...  \n3  철골철근 콘크리트 구조는 강철 골조와 강철 철근 콘크리트를 사용하여 건축물을 구성하...  \n4  철골구조는 일반철골구조와 경량철골구조 두 가지 방식이 주로 사용됩니다 일반철골구조는...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>질문_1</th>\n      <th>질문_2</th>\n      <th>category</th>\n      <th>답변_1</th>\n      <th>답변_2</th>\n      <th>답변_3</th>\n      <th>답변_4</th>\n      <th>답변_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRAIN_000</td>\n      <td>면진장치가 뭐야</td>\n      <td>면진장치에 사용되는 주요 기술은 무엇인가요</td>\n      <td>건축구조</td>\n      <td>면진장치란 지반에서 오는 진동 에너지를 흡수하여 건물에 주는 진동을 줄여주는 진동 ...</td>\n      <td>면진장치란 건물의 지반에서 발생하는 진동 에너지를 흡수하여 건물을 보호하고 진동을 ...</td>\n      <td>면진장치란 지반으로부터 발생하는 진동 에너지를 흡수하여 건물에 전달되는 진동을 줄여...</td>\n      <td>면진장치는 건물의 지반으로부터 오는 진동 에너지를 흡수하여 건물에 전달되는 진동을 ...</td>\n      <td>면진장치는 건물에 오는 지반 진동의 영향을 최대한으로 흡수하여 건물에 전달되는 진동...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRAIN_001</td>\n      <td>내진설계의 종류 알려줘</td>\n      <td>내진설계에는 종류가 있는지 자세히 알려주실 있나요</td>\n      <td>건축구조</td>\n      <td>내진 설계의 종류로 내진구조 제진구조 면진구조가 있습니다</td>\n      <td>내진설계에는 내진구조 제진구조 면진구조가 있습니다 내진구조는 건물 구조물이 지진의 ...</td>\n      <td>내진설계에는 주로 내진구조 제진구조 면진구조의 세 가지 종류가 있습니다 이들은 지진...</td>\n      <td>내진설계에는 주로 내진구조 제진구조 면진구조가 사용됩니다 내진구조는 건물 구조물 전...</td>\n      <td>내진 설계에는 다양한 종류가 있지만 대표적으로 내진구조 제진구조 면진구조가 있습니다...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRAIN_002</td>\n      <td>철골구조의 장점이 뭐야</td>\n      <td>철골구조의 장점을 알려줘</td>\n      <td>건축구조</td>\n      <td>철골구조는 건물의 외벽에는 그다지 하중이 걸리지 고층 건물의 건축이 가능한 것이 장...</td>\n      <td>철골구조의 장점은 건물의 외벽에는 그다지 하중이 걸리지 고층 건물의 건축이 가능하다...</td>\n      <td>철골구조의 장점은 건물의 외벽에 하중이 적게 걸리기 고층 건물의 건축이 용이하다는 ...</td>\n      <td>철골구조의 장점은 건물의 외벽이 하중이 걸리지 않아 공간 활용이 용이하고 고층 건물...</td>\n      <td>철골구조의 장점은 건물의 외벽에 하중이 크게 걸리지 않아 고층 건물을 건축할 있는 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRAIN_003</td>\n      <td>철골철근 콘크리트 구조가 뭐야</td>\n      <td>철골철근 콘크리트 구조의 장점과 단점에는 무엇이 있을까요</td>\n      <td>건축구조</td>\n      <td>철근철골콘크리트는 철골과 철근 콘크리트를 사용하는 건축 구조입니다 철골은 강하고 가...</td>\n      <td>철골철근콘크리트 구조는 건축물을 지탱하는 주요 구조물인 철골과 철근 콘크리트를 사용...</td>\n      <td>철골철근 콘크리트 구조는 건축물을 지탱하기 위한 구조물에서 사용되는 방식으로 철골을...</td>\n      <td>철골철근콘크리트 구조는 철골과 철근 콘크리트를 사용하여 만들어지는 건축 구조입니다 ...</td>\n      <td>철골철근 콘크리트 구조는 강철 골조와 강철 철근 콘크리트를 사용하여 건축물을 구성하...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRAIN_004</td>\n      <td>철골구조는 방식이 있어</td>\n      <td>철골구조의 다양한 방식이 무엇인가요</td>\n      <td>건축구조</td>\n      <td>철골구조는 일반철골구조와 경량철골구조가 있습니다</td>\n      <td>철골구조는 일반철골구조와 경량철골구조가 있습니다 일반철골구조는 주로 대형 건물이나 ...</td>\n      <td>철골구조는 주로 일반철골구조와 경량철골구조로 나뉘어집니다 이들은 건축 시스템에 선적...</td>\n      <td>철골구조는 주로 일반철골구조와 경량철골구조로 구분됩니다 최근에는 고층 건물에서 사용...</td>\n      <td>철골구조는 일반철골구조와 경량철골구조 두 가지 방식이 주로 사용됩니다 일반철골구조는...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.357434Z",
          "iopub.execute_input": "2024-02-27T08:52:47.357805Z",
          "iopub.status.idle": "2024-02-27T08:52:47.368729Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.357763Z",
          "shell.execute_reply": "2024-02-27T08:52:47.367731Z"
        },
        "trusted": true,
        "id": "SAQWxewJYMLB",
        "outputId": "0551e91a-470d-4bb1-947b-318017f03e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "id          0\n질문_1        0\n질문_2        0\ncategory    0\n답변_1        0\n답변_2        0\n답변_3        0\n답변_4        0\n답변_5        0\ndtype: int64"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#skt_tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2', eos_token='</s>', pad_token='<pad>')\n",
        "#kakao_tokenizer = AutoTokenizer.from_pretrained('kakaobrain/kogpt', eos_token='[EOS]')\n",
        "polyglot_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-12.8B\", eos_token='<|endoftext|>')\n",
        "# Klm_tokenizer = transformers.AutoTokenizer.from_pretrained(\"quantumaikr/KoreanLM-1.5b\", eos_token='</s>')\n",
        "# dot_42_tokenizer = transformers.AutoTokenizer.from_pretrained(\"42dot/42dot_LLM-PLM-1.3B\", eos_token=\"<|endoftext|>\")\n",
        "\n",
        "#polyglot_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-3.8b\", eos_token='<|endoftext|>')\n",
        "#electra_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-generator\", eos_token='<|endoftext|>')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:47.369703Z",
          "iopub.execute_input": "2024-02-27T08:52:47.370012Z",
          "iopub.status.idle": "2024-02-27T08:52:55.69952Z",
          "shell.execute_reply.started": "2024-02-27T08:52:47.369983Z",
          "shell.execute_reply": "2024-02-27T08:52:55.698597Z"
        },
        "trusted": true,
        "id": "SpEfAm_jYMLB",
        "outputId": "aa749b42-fb22-4e2c-9205-3b7f70e4fb09",
        "colab": {
          "referenced_widgets": [
            "8290167b785d4b41b8af42dd6e006c03",
            "96b5fef676654f53ab354636c0e2b52d",
            "49dece7bd24548f4b7fc580e4015f1ec",
            "5cded41eb0e846ab81288036325baecf",
            "806db11a6daf40d9bbe6230813fef75c",
            "6dda291dc0d04b32b835b414ee795041",
            "ed8defa473f5456b928433cf89389e3e",
            "3a34c8967e5342de9af679df66eedead",
            "e176c2bbecae4262a81f946f2a32a68a",
            "8c312ae8e60949d1bcf0f149bd50b63d",
            "28a30b6ff3de4182ad82828d6a391806",
            "20b196fad9a2420888c06dfcac9a1001",
            "865fd6682ff847b4b33a1a9ecc9271ca",
            "91807a5a1c6c407ba6a4c51695f8dc2c",
            "d08cbf1d13604c928fe0e598a5b1d5d0",
            "c6a310df1d6844e7a4509582f7e896d3",
            "3cbb42c64314412681d9b390132e2305"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8290167b785d4b41b8af42dd6e006c03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96b5fef676654f53ab354636c0e2b52d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \nThe class this function is called from is 'PreTrainedTokenizerFast'.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/164 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49dece7bd24548f4b7fc580e4015f1ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.65M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cded41eb0e846ab81288036325baecf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/185 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "806db11a6daf40d9bbe6230813fef75c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/780 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6dda291dc0d04b32b835b414ee795041"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/568k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed8defa473f5456b928433cf89389e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/354k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a34c8967e5342de9af679df66eedead"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.47M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e176c2bbecae4262a81f946f2a32a68a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c312ae8e60949d1bcf0f149bd50b63d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/216 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28a30b6ff3de4182ad82828d6a391806"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20b196fad9a2420888c06dfcac9a1001"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/871k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "865fd6682ff847b4b33a1a9ecc9271ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/529k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91807a5a1c6c407ba6a4c51695f8dc2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/2.26M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d08cbf1d13604c928fe0e598a5b1d5d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6a310df1d6844e7a4509582f7e896d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cbb42c64314412681d9b390132e2305"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "formatted_data = []\n",
        "for _, row in tqdm(train.iterrows()):\n",
        "    for q_col in ['질문_1', '질문_2']:\n",
        "        for a_col in ['답변_1', '답변_2', '답변_3', '답변_4', '답변_5']:\n",
        "            # 질문과 답변 쌍을 </s> token으로 연결\n",
        "            input_text = row[q_col] + skt_tokenizer.eos_token + row[a_col]\n",
        "            input_ids = skt_tokenizer.encode(input_text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
        "            #input_ids = skt_tokenizer.encode(input_text, return_tensors='pt')\n",
        "            formatted_data.append(input_ids)\n",
        "print('Done.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:55.834933Z",
          "iopub.execute_input": "2024-02-27T08:52:55.835202Z",
          "iopub.status.idle": "2024-02-27T08:52:58.168812Z",
          "shell.execute_reply.started": "2024-02-27T08:52:55.835179Z",
          "shell.execute_reply": "2024-02-27T08:52:58.167959Z"
        },
        "trusted": true,
        "id": "Ubzf37WtYMLC",
        "outputId": "4d0044a0-505b-4af2-c54c-5400552026f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "644it [00:02, 277.30it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Done.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_data = torch.cat(formatted_data, dim=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:58.20773Z",
          "iopub.execute_input": "2024-02-27T08:52:58.207986Z",
          "iopub.status.idle": "2024-02-27T08:52:58.211892Z",
          "shell.execute_reply.started": "2024-02-27T08:52:58.207964Z",
          "shell.execute_reply": "2024-02-27T08:52:58.210936Z"
        },
        "trusted": true,
        "id": "q_iCATr8YMLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_data[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:58.212863Z",
          "iopub.execute_input": "2024-02-27T08:52:58.213138Z",
          "iopub.status.idle": "2024-02-27T08:52:58.222193Z",
          "shell.execute_reply.started": "2024-02-27T08:52:58.213116Z",
          "shell.execute_reply": "2024-02-27T08:52:58.221174Z"
        },
        "trusted": true,
        "id": "0pwaznt8YMLC",
        "outputId": "e96d41b7-7e3d-4f7b-9759-449ccc6f111f"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "tensor([[ 9411,  8265, 37765, 46651,  7991,     1,  9411,  8265, 20725,  7374,\n          9027,  7599,  9023, 14472, 15898, 14820, 33220, 36928, 10764,  9166,\n         11818, 28037, 10090, 15898, 34062, 20725,  8153,  7172,  7182]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'LR':5e-5,\n",
        "    'EPOCHS':20,\n",
        "    'BATCH_SIZE':8\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:58.223251Z",
          "iopub.execute_input": "2024-02-27T08:52:58.223507Z",
          "iopub.status.idle": "2024-02-27T08:52:58.230766Z",
          "shell.execute_reply.started": "2024-02-27T08:52:58.223485Z",
          "shell.execute_reply": "2024-02-27T08:52:58.229939Z"
        },
        "trusted": true,
        "id": "p9ZsQTozYMLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Wandb\n",
        "import wandb\n",
        "import random\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "    project = 'LLM_deaplearning',\n",
        "    config={\n",
        "    \"learning_rate\": CFG['LR'],\n",
        "    \"architecture\": \"skt/kogpt2-base-v2\",\n",
        "    \"dataset\": formatted_data,\n",
        "    \"epochs\": CFG['EPOCHS'],\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:52:58.231936Z",
          "iopub.execute_input": "2024-02-27T08:52:58.232231Z",
          "iopub.status.idle": "2024-02-27T08:59:48.512018Z",
          "shell.execute_reply.started": "2024-02-27T08:52:58.232209Z",
          "shell.execute_reply": "2024-02-27T08:59:48.511158Z"
        },
        "trusted": true,
        "id": "0uUpYA6rYMLC",
        "outputId": "cf401f44-63ba-4223-9eb9-66ae4d34521e",
        "colab": {
          "referenced_widgets": [
            "5735c3985242436f8de126250be0f4b6"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········································\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mttony0321\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113116699999006, max=1.0…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5735c3985242436f8de126250be0f4b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240227_085904-ws2dnlny</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/ttony0321/LLM_deaplearning/runs/ws2dnlny' target=\"_blank\">iconic-snow-18</a></strong> to <a href='https://wandb.ai/ttony0321/LLM_deaplearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/ttony0321/LLM_deaplearning' target=\"_blank\">https://wandb.ai/ttony0321/LLM_deaplearning</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/ttony0321/LLM_deaplearning/runs/ws2dnlny' target=\"_blank\">https://wandb.ai/ttony0321/LLM_deaplearning/runs/ws2dnlny</a>"
          },
          "metadata": {}
        },
        {
          "execution_count": 27,
          "output_type": "execute_result",
          "data": {
            "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ttony0321/LLM_deaplearning/runs/ws2dnlny?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
            "text/plain": "<wandb.sdk.wandb_run.Run at 0x77feacd78040>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('EleutherAI/polyglot-ko-12.8b')\n",
        "print(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:59:48.518528Z",
          "iopub.execute_input": "2024-02-27T08:59:48.519181Z",
          "iopub.status.idle": "2024-02-27T08:59:48.523934Z",
          "shell.execute_reply.started": "2024-02-27T08:59:48.519146Z",
          "shell.execute_reply": "2024-02-27T08:59:48.522944Z"
        },
        "trusted": true,
        "id": "NXr4AoLPYMLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#module 확인"
      ],
      "metadata": {
        "id": "eZn6dqAQbVox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "def find_all_linear_names(model):\n",
        "\n",
        "    cls = bnb.nn.Linear4bit\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)\n",
        "\n",
        "lora_modules = find_all_linear_names(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:59:48.52539Z",
          "iopub.execute_input": "2024-02-27T08:59:48.52595Z",
          "iopub.status.idle": "2024-02-27T08:59:48.53512Z",
          "shell.execute_reply.started": "2024-02-27T08:59:48.525919Z",
          "shell.execute_reply": "2024-02-27T08:59:48.534297Z"
        },
        "trusted": true,
        "id": "pZuG67uRYMLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lora_modules)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:59:48.536298Z",
          "iopub.execute_input": "2024-02-27T08:59:48.537138Z",
          "iopub.status.idle": "2024-02-27T08:59:48.544918Z",
          "shell.execute_reply.started": "2024-02-27T08:59:48.537106Z",
          "shell.execute_reply": "2024-02-27T08:59:48.544054Z"
        },
        "trusted": true,
        "id": "ZLQ5T6G_YMLC"
      },
      "execution_count": null,
      "outputs": []
    },
    
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "# 4bit quantization\n",
        "# load model\n",
        "model_id = 'EleutherAI/polyglot-ko-12.8b'\n",
        "\n",
        "\n",
        "\n",
        "#setting 4bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\":0},\n",
        ")\n",
        "\n",
        "#target modules support torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\n",
        "model.gradient_checkpointing_enable()#GPU 사용량 줄이고 연산을 더하는 코드\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "config = LoraConfig(#\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    #target_modules=[\"query_key_value\"],\n",
        "    target_modules=[\n",
        "    \"query_key_value\",\n",
        "    \"dense\",\n",
        "    \"dense_h_to_4h\",\n",
        "    \"dense_4h_to_h\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr = CFG['LR'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:59:51.587747Z",
          "iopub.execute_input": "2024-02-27T08:59:51.5884Z",
          "iopub.status.idle": "2024-02-27T08:59:51.598062Z",
          "shell.execute_reply.started": "2024-02-27T08:59:51.58837Z",
          "shell.execute_reply": "2024-02-27T08:59:51.596873Z"
        },
        "trusted": true,
        "id": "90ZFmOZuYMLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4bit quantization\n",
        "import transformers\n",
        "import wandb\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=formatted_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        report_to='none',\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=4,\n",
        "        #max_steps=50,\n",
        "        learning_rate=CFG['LR'],\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        output_dir=\"/kaggle/working/outputs\",\n",
        "        #optim=\"paged_adamw_8bit\"\n",
        "        optim='adamw_8bit'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(polyglot_tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-27T08:59:51.599781Z",
          "iopub.execute_input": "2024-02-27T08:59:51.600515Z",
          "iopub.status.idle": "2024-02-27T08:59:51.610519Z",
          "shell.execute_reply.started": "2024-02-27T08:59:51.600484Z",
          "shell.execute_reply": "2024-02-27T08:59:51.609668Z"
        },
        "trusted": true,
        "id": "88tIYRINYMLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working/save\n",
        "!ls"
      ],
      "metadata": {
        "trusted": true,
        "id": "57H5wuFiYMLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):\n",
        "    html = '<a href={filename}>{title}</a>'\n",
        "    html = html.format(title=title,filename=filename)\n",
        "    return HTML(html)\n",
        "\n",
        "# create a link to download the dataframe which was saved with .to_csv method\n",
        "create_download_link(filename='model.safetensors')\n",
        "from IPython.display import FileLink\n",
        "#FileLink(r'model.safetensors')"
      ],
      "metadata": {
        "trusted": true,
        "id": "NDAJTOj9YMLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import subprocess\n",
        "from IPython.display import FileLink, display\n",
        "def download_file(path, download_file_name):\n",
        "    os.chdir('/kaggle/working/')\n",
        "    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n",
        "    command = f\"zip {zip_name} {path} -r\"\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(\"Unable to run zip command!\")\n",
        "        print(result.stderr)\n",
        "        return\n",
        "    display(FileLink(f'{download_file_name}.zip'))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "rzr1nBe1YMLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_file('/kaggle/working/save/', 'model.safetensors')"
      ],
      "metadata": {
        "trusted": true,
        "id": "RIV5YKxNYMLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 Fine-tuned 모델과 토크나이저 불러오기\n",
        "model_dir = \"/kaggle/working/save\"\n",
        "#model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir, quantized=True)\n",
        "model.to(device)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_dir)\n",
        "\n",
        "# Inference를 위한 test.csv 파일 로드\n",
        "test = pd.read_csv('/kaggle/input/llm-datasets/test.csv')\n",
        "\n",
        "# test.csv의 '질문'에 대한 '답변'을 저장할 리스트\n",
        "preds = []\n",
        "\n",
        "# '질문' 컬럼의 각 질문에 대해 답변 생성\n",
        "for test_question in tqdm(test['질문']):\n",
        "    # 입력 텍스트를 토큰화하고 모델 입력 형태로 변환\n",
        "    input_ids = tokenizer.encode(test_question + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # 답변 생성\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=input_ids.to(device),\n",
        "        max_length=300,\n",
        "        temperature=0.9,\n",
        "        top_k=1,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # 생성된 텍스트(답변) 저장\n",
        "    for generated_sequence in output_sequences:\n",
        "        full_text = tokenizer.decode(generated_sequence, skip_special_tokens=False)\n",
        "        # 질문과 답변의 사이를 나타내는 eos_token (</s>)를 찾아, 이후부터 출력\n",
        "        answer_start = full_text.find(tokenizer.eos_token) + len(tokenizer.eos_token)\n",
        "        answer_only = full_text[answer_start:].strip()\n",
        "        answer_only = answer_only.replace('\\n', ' ')\n",
        "        preds.append(answer_only)"
      ],
      "metadata": {
        "trusted": true,
        "id": "aZXvM7EtYMLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z63aRwp-YMLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 데이터셋의 모든 질의에 대한 답변으로부터 512 차원의 Embedding Vector 추출\n",
        "# 평가를 위한 Embedding Vector 추출에 활용하는 모델은 'distiluse-base-multilingual-cased-v1' 이므로 반드시 확인해주세요.\n",
        "from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2\n",
        "\n",
        "# Embedding Vector 추출에 활용할 모델(distiluse-base-multilingual-cased-v1) 불러오기\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "# 생성한 모든 응답(답변)으로부터 Embedding Vector 추출\n",
        "pred_embeddings = model.encode(preds)\n",
        "pred_embeddings.shape\n",
        "submit = pd.read_csv('/kaggle/input/llm-datasets/sample_submission.csv')\n",
        "# 제출 양식 파일(sample_submission.csv)을 활용하여 Embedding Vector로 변환한 결과를 삽입\n",
        "submit.iloc[:,1:] = pred_embeddings\n",
        "submit.head()\n",
        "# 리더보드 제출을 위한 csv파일 생성\n",
        "submit.to_csv('/kaggle/working/baseline_submit.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "E_j3Y1BBYMLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZnDd-QM5YMLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
